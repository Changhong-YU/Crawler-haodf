{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28f0407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 好大夫 https://www.haodf.com\n",
    "## 爬取问诊信息\n",
    "## 医生、病例、建议、对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ffebe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import os,sys\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def flushPrint(s):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('%s' % s)\n",
    "    sys.stdout.flush()\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9deb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '../Data/webpage/'\n",
    "datapath = '../Data/webpage/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140ead31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver输入参数\n",
    "options=webdriver.ChromeOptions()\n",
    "out_path='/Users/yuchanghong/Desktop/haodf/Data/webpage/'\n",
    "prefs={'profile.default_content_settings.popups': 0, 'download.default_directory': out_path}\n",
    "options.add_experimental_option('prefs', prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021a9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_files(directory):\n",
    "    # 获取目录下所有文件\n",
    "    all_files = os.listdir(directory)\n",
    "\n",
    "    # 使用正则表达式匹配以数字开头的HTML文件\n",
    "    pattern = re.compile(r'^\\d.*\\.html$')\n",
    "    html_files = [file for file in all_files if pattern.match(file)]\n",
    "    html_files_without_extension = [os.path.splitext(file)[0] for file in html_files]\n",
    "\n",
    "\n",
    "    return html_files_without_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa2cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df, file, outpath,suffix):\n",
    "    # 创建最终的文件名\n",
    "    filename = f\"{file}_{suffix}\"\n",
    "\n",
    "    # 存为 Excel 文件（使用 utf-16 编码）\n",
    "    excel_filepath = os.path.join(outpath, f\"{filename}.xlsx\")\n",
    "    df.to_excel(excel_filepath, index=False, encoding='utf-16')\n",
    "\n",
    "    # 存为 Python 二进制文件（Pickle 格式，也使用 utf-16 编码）\n",
    "    pickle_filepath = os.path.join(outpath, f\"{filename}.pkl\")\n",
    "    df.to_pickle(pickle_filepath, protocol=5)  # protocol=5 表示使用最高的 Pickle 协议版本\n",
    "    \n",
    "    #print(excel_filepath, 'saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb483d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 爬取网页链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec19e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/p1s2dy_d55zbvx8s9j4nd_x00000gn/T/ipykernel_1520/1871793919.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
      "/var/folders/r7/p1s2dy_d55zbvx8s9j4nd_x00000gn/T/ipykernel_1520/1871793919.py:1: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b3c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_prefix = \"https://www.haodf.com/bingcheng/list.html?p=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc42b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content_prefix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r7/p1s2dy_d55zbvx8s9j4nd_x00000gn/T/ipykernel_1140/1587128774.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcontent_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'content_prefix' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    content_num = str(i+1)\n",
    "    content = content_prefix + content_num\n",
    "    \n",
    "    link=content\n",
    "    driver.get(link)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    html_content = driver.page_source\n",
    "    fl_name = \"content_\" + content_num\n",
    "    with open(f'{outpath}/{fl_name}.html', 'w', encoding='utf-16') as file:\n",
    "        file.write(html_content)  \n",
    "print(\"content page saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    content_num = str(i+1)\n",
    "    fl_name = \"content_\" + content_num\n",
    "    html_file_path = f'{outpath}/{fl_name}.html'\n",
    "    with open(html_file_path, 'r', encoding='utf-16') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = soup.find_all('span', class_='fl') \n",
    "    # Extract and print the URLs\n",
    "    for link in links:\n",
    "        a_tag = link.find('a')\n",
    "        if a_tag:\n",
    "            url = a_tag.get('href')\n",
    "            #print(url)\n",
    "            l.append(url)\n",
    "    if i % 20 == 0:\n",
    "        print(''.join(['第',str(i+1),\"个网页中的链接已储存，现在共有\",str(len(l)),'个链接。']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a09cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a NumPy file\n",
    "np.save(f'{outpath}/content.npy', l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9118bd",
   "metadata": {},
   "source": [
    "## 下载网页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde4c2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n",
      "['https://www.haodf.com/bingcheng/8897290087.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289968.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289927.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289925.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289914.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289904.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289896.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289892.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289879.html'\n",
      " 'https://www.haodf.com/bingcheng/8897289875.html']\n"
     ]
    }
   ],
   "source": [
    "l = np.load(f'{outpath}/content.npy')\n",
    "print(len(l))\n",
    "print(l[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3d24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if the \"没有更多交流了~\" message is displayed\n",
    "def no_more_messages():\n",
    "    try:\n",
    "        # Check if the element is present on the page\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'msg-notext'))\n",
    "        )\n",
    "        # Check if the element contains the specified text\n",
    "        return \"没有更多交流了~\" in element.text\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Define a function to click the \"查看更多交流\" button\n",
    "def click_more_button():\n",
    "    try:\n",
    "        # Click the \"查看更多交流\" button\n",
    "        more_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'msg-more-link-text'))\n",
    "        )\n",
    "        more_button.click()\n",
    "        time.sleep(8)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b1ec4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/p1s2dy_d55zbvx8s9j4nd_x00000gn/T/ipykernel_12386/2834877089.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
      "/var/folders/r7/p1s2dy_d55zbvx8s9j4nd_x00000gn/T/ipykernel_12386/2834877089.py:1: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install(),chrome_options=options)\n",
    "link=\"https://passport.haodf.com/nusercenter/showlogin\"\n",
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27eb20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually login in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a5b0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数字文件名列表长度： 300\n"
     ]
    }
   ],
   "source": [
    "l_num = np.load(f'{outpath}/num.npy')\n",
    "print('数字文件名列表长度：',len(l_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b67421f-47fc-4d08-80a6-19d7a89a1a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_num = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4c430ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 663\n"
     ]
    }
   ],
   "source": [
    "start = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3f5b72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下载第 663 个网页链接。\n",
      "正在下载第 664 个网页链接。\n",
      "正在下载第 665 个网页链接。\n",
      "正在下载第 666 个网页链接。\n",
      "正在下载第 667 个网页链接。\n",
      "正在下载第 668 个网页链接。\n",
      "正在下载第 669 个网页链接。\n",
      "正在下载第 670 个网页链接。\n",
      "正在下载第 671 个网页链接。\n",
      "正在下载第 672 个网页链接。\n",
      "正在下载第 673 个网页链接。\n",
      "正在下载第 674 个网页链接。\n",
      "正在下载第 675 个网页链接。\n",
      "正在下载第 676 个网页链接。\n",
      "正在下载第 677 个网页链接。\n",
      "https://www.haodf.com/bingcheng/8897283756.html  failed.\n",
      "正在下载第 678 个网页链接。\n",
      "正在下载第 679 个网页链接。\n",
      "正在下载第 680 个网页链接。\n",
      "正在下载第 681 个网页链接。\n",
      "正在下载第 682 个网页链接。\n",
      "正在下载第 683 个网页链接。\n",
      "正在下载第 684 个网页链接。\n",
      "正在下载第 685 个网页链接。\n",
      "正在下载第 686 个网页链接。\n",
      "正在下载第 687 个网页链接。\n",
      "正在下载第 688 个网页链接。\n",
      "正在下载第 689 个网页链接。\n",
      "正在下载第 690 个网页链接。\n",
      "正在下载第 691 个网页链接。\n",
      "正在下载第 692 个网页链接。\n",
      "正在下载第 693 个网页链接。\n",
      "正在下载第 694 个网页链接。\n",
      "正在下载第 695 个网页链接。\n",
      "正在下载第 696 个网页链接。\n",
      "正在下载第 697 个网页链接。\n",
      "正在下载第 698 个网页链接。\n",
      "正在下载第 699 个网页链接。\n",
      "正在下载第 700 个网页链接。\n",
      "正在下载第 701 个网页链接。\n",
      "正在下载第 702 个网页链接。\n",
      "正在下载第 703 个网页链接。\n",
      "正在下载第 704 个网页链接。\n",
      "正在下载第 705 个网页链接。\n",
      "正在下载第 706 个网页链接。\n",
      "正在下载第 707 个网页链接。\n",
      "正在下载第 708 个网页链接。\n",
      "正在下载第 709 个网页链接。\n",
      "正在下载第 710 个网页链接。\n",
      "正在下载第 711 个网页链接。\n",
      "正在下载第 712 个网页链接。\n",
      "正在下载第 713 个网页链接。\n",
      "https://www.haodf.com/bingcheng/8897283508.html  failed.\n",
      "正在下载第 714 个网页链接。\n",
      "正在下载第 715 个网页链接。\n",
      "正在下载第 716 个网页链接。\n",
      "正在下载第 717 个网页链接。\n",
      "正在下载第 718 个网页链接。\n",
      "正在下载第 719 个网页链接。\n",
      "正在下载第 720 个网页链接。\n",
      "正在下载第 721 个网页链接。\n",
      "正在下载第 722 个网页链接。\n",
      "正在下载第 723 个网页链接。\n",
      "正在下载第 724 个网页链接。\n",
      "正在下载第 725 个网页链接。\n",
      "正在下载第 726 个网页链接。\n",
      "正在下载第 727 个网页链接。\n",
      "正在下载第 728 个网页链接。\n",
      "正在下载第 729 个网页链接。\n",
      "正在下载第 730 个网页链接。\n",
      "正在下载第 731 个网页链接。\n",
      "正在下载第 732 个网页链接。\n",
      "正在下载第 733 个网页链接。\n",
      "正在下载第 734 个网页链接。\n",
      "正在下载第 735 个网页链接。\n",
      "正在下载第 736 个网页链接。\n",
      "正在下载第 737 个网页链接。\n",
      "正在下载第 738 个网页链接。\n",
      "正在下载第 739 个网页链接。\n",
      "正在下载第 740 个网页链接。\n",
      "https://www.haodf.com/bingcheng/8897283364.html  failed.\n",
      "正在下载第 741 个网页链接。\n",
      "正在下载第 742 个网页链接。\n",
      "正在下载第 743 个网页链接。\n",
      "正在下载第 744 个网页链接。\n",
      "正在下载第 745 个网页链接。\n",
      "正在下载第 746 个网页链接。\n",
      "正在下载第 747 个网页链接。\n",
      "正在下载第 748 个网页链接。\n",
      "正在下载第 749 个网页链接。\n",
      "正在下载第 750 个网页链接。\n",
      "正在下载第 751 个网页链接。\n",
      "正在下载第 752 个网页链接。\n",
      "正在下载第 753 个网页链接。\n",
      "正在下载第 754 个网页链接。\n",
      "正在下载第 755 个网页链接。\n",
      "正在下载第 756 个网页链接。\n",
      "正在下载第 757 个网页链接。\n",
      "正在下载第 758 个网页链接。\n",
      "正在下载第 759 个网页链接。\n",
      "正在下载第 760 个网页链接。\n",
      "正在下载第 761 个网页链接。\n",
      "正在下载第 762 个网页链接。\n",
      "663 - 763 are saved.\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(start),int(start)+100):\n",
    "    link = l[i]\n",
    "    num = link.split('/')[-1].split('.')[0]\n",
    "    l_num = np.append(l_num, num)\n",
    "    \n",
    "    print(\"正在下载第\",i,'个网页链接。')\n",
    "\n",
    "    driver.get(link)\n",
    "    # Keep clicking the button until \"没有更多交流了~\" is displayed\n",
    "    f = 0\n",
    "    while not no_more_messages():\n",
    "        click_more_button()\n",
    "        f += 1\n",
    "        if f > 5:\n",
    "            print(link,' failed.')\n",
    "            break\n",
    "            \n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    with open(f'{outpath}/{num}.html', 'w', encoding='utf-16') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "print(start,'-',int(start)+100, 'are saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8fb46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9b0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da387fbb",
   "metadata": {},
   "source": [
    "## 提取信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "187346b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你的目录路径\n",
    "directory_path = '../Data/webpage'\n",
    "\n",
    "# 获取HTML文件列表\n",
    "html_files_list = get_html_files(directory_path)\n",
    "\n",
    "np.save(f'{outpath}/num.npy', html_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fbb7953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    }
   ],
   "source": [
    "l_num = np.load(f'{outpath}/num.npy')\n",
    "print(len(l_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c06ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_num_flaw = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12439635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#l_num_flaw = np.load(f'{outpath}/num_flaw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc6fb056-75ea-405a-8e7e-c1be173214f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8897283364' '8897283508' '8897283756' '8897285638' '8897286429'\n",
      " '8897287010' '8897287389' '8897287926' '8897288332']\n"
     ]
    }
   ],
   "source": [
    "#print(l_num_flaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e5c568",
   "metadata": {},
   "source": [
    "### info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "207582e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_cat(df,cat):\n",
    "    df.insert(0, 'category', cat)\n",
    "    df.insert(1, 'num', range(1, len(df) + 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a360fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFO\n",
    "def meta(soup):\n",
    "    # Extract meta tags with specified attributes\n",
    "    meta_tags = soup.find_all('meta', {'name': 'description'})\n",
    "    meta_tags += soup.find_all('meta', {'name': 'keywords'})\n",
    "    meta_tags += soup.find_all('meta', {'name': 'mobile-agent'})\n",
    "    meta_tags += soup.find_all('meta', {'property': 'og:url'})\n",
    "    meta_tags += soup.find_all('meta', {'property': 'og:type'})\n",
    "    meta_tags += soup.find_all('meta', {'property': 'og:title'})\n",
    "    meta_tags += soup.find_all('meta', {'property': 'og:release_date'})\n",
    "    meta_tags += soup.find_all('meta', {'property': 'og:description'})\n",
    "    \n",
    "    # Create a DataFrame from the extracted meta tags\n",
    "    df_meta = pd.DataFrame([(tag.attrs.get('name') or tag.attrs.get('property'), tag.attrs.get('content')) for tag in meta_tags],\n",
    "                       columns=['attribute', 'value'])\n",
    "    df_meta = insert_cat(df_meta,'meta')\n",
    "    return df_meta\n",
    "\n",
    "def disease_info(soup):\n",
    "    # Extracting information from the HTML (Disease Info)\n",
    "    disease_info = soup.select_one('.diseaseinfo')\n",
    "    info_items = disease_info.find_all(['span', 'br'])\n",
    "    \n",
    "    data_disease_info = {'attribute': [], 'value': []}\n",
    "    current_attribute = None\n",
    "    \n",
    "    for item in info_items:\n",
    "        if item.name == 'span' and 'info3-title' in item.get('class', []):\n",
    "            current_attribute = item.text.strip()\n",
    "        elif item.name == 'span' and 'info3-value' in item.get('class', []):\n",
    "            # If the attribute already exists, concatenate the new value with a newline\n",
    "            if current_attribute in data_disease_info['attribute']:\n",
    "                idx = data_disease_info['attribute'].index(current_attribute)\n",
    "                data_disease_info['value'][idx] += '\\n' + item.text.strip()\n",
    "            else:\n",
    "                data_disease_info['attribute'].append(current_attribute)\n",
    "                data_disease_info['value'].append(item.text.strip())\n",
    "    \n",
    "    # Creating a DataFrame for Disease Info\n",
    "    df_disease_info = pd.DataFrame(data_disease_info)\n",
    "    df_disease_info = insert_cat(df_disease_info,'disease_info')\n",
    "    return df_disease_info\n",
    "\n",
    "def suggestions(soup):\n",
    "    # Extracting information from the HTML (Suggestions)\n",
    "    suggestions_sections = soup.select('.suggestions-text')\n",
    "    \n",
    "    data_suggestions = {'attribute': [], 'value': []}\n",
    "    current_attribute_suggestions = None\n",
    "    \n",
    "    for suggestions_section in suggestions_sections:\n",
    "        curr_head = suggestions_section.select_one('.curr-head')\n",
    "        if curr_head:\n",
    "            current_attribute_suggestions = curr_head.text.strip()\n",
    "        suggestions_text_value = suggestions_section.select_one('.suggestions-text-value')\n",
    "        if suggestions_text_value:\n",
    "            # If the attribute already exists, concatenate the new value with a newline\n",
    "            if current_attribute_suggestions in data_suggestions['attribute']:\n",
    "                idx = data_suggestions['attribute'].index(current_attribute_suggestions)\n",
    "                data_suggestions['value'][idx] += '\\n' + suggestions_text_value.text.strip()\n",
    "            else:\n",
    "                data_suggestions['attribute'].append(current_attribute_suggestions)\n",
    "                data_suggestions['value'].append(suggestions_text_value.text.strip())\n",
    "    \n",
    "    # Creating a DataFrame for Suggestions\n",
    "    df_suggestions = pd.DataFrame(data_suggestions)\n",
    "    df_suggestions = insert_cat(df_suggestions,'suggestions')\n",
    "    return df_suggestions\n",
    "\n",
    "def doctor_info(soup):\n",
    "    # Extract relevant information (Doctor Info)\n",
    "    info_text_name = soup.find('span', class_='info-text-name').text.strip()\n",
    "    info_text_grade = soup.find('span', class_='info-text-grade').text.strip()\n",
    "    hospital = soup.find('a', class_='hospital').text.strip()\n",
    "    faculty = soup.find('a', class_='faculty').text.strip()\n",
    "    \n",
    "    # Creating a DataFrame for Doctor Info\n",
    "    df_doctor_info = pd.DataFrame({'attribute': ['info-text-name', 'info-text-grade', 'hospital', 'faculty'],\n",
    "                                'value': [info_text_name, info_text_grade, hospital, faculty]})\n",
    "    \n",
    "        # Extract additional relevant information\n",
    "    add_info = soup.find_all('div', class_='item-title')\n",
    "    add_info_value = soup.find_all('div', class_='item-detail')\n",
    "    \n",
    "    # Creating a DataFrame for Additional Info\n",
    "    df_additional = pd.DataFrame({'attribute': [add_info[0].text.strip(), add_info[1].text.strip()],\n",
    "                              'value': [add_info_value[0].text.strip(), add_info_value[1].text.strip()]})\n",
    "    \n",
    "    \n",
    "    # Extract the HTML content as a string\n",
    "    html_string = str(soup)\n",
    "    # Define the regex pattern\n",
    "    pattern = r'href=\"(https://www\\.haodf\\.com/doctor/\\d+\\.html)\"'\n",
    "    # Use re.findall to extract the URL\n",
    "    match = re.search(pattern, html_string)\n",
    "    url = match.group(1)\n",
    "        \n",
    "    # Define the regex pattern to extract the doc_id\n",
    "    doc_id_pattern = r'/doctor/(\\d+)\\.html'\n",
    "    match = re.search(doc_id_pattern, url)\n",
    "    \n",
    "    # Extract doc_id if a match is found\n",
    "    doc_id = match.group(1) if match else None\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    data = {'attribute': ['url', 'doc_id'],\n",
    "            'value': [url, doc_id]}\n",
    "    \n",
    "    df_docid = pd.DataFrame(data)\n",
    "\n",
    "    \n",
    "    df_doctor_info = pd.concat([df_doctor_info, df_additional, df_docid],ignore_index=True)\n",
    "    \n",
    "    df_doctor_info = insert_cat(df_doctor_info,'doctor_info')\n",
    "    \n",
    "    \n",
    "    return df_doctor_info\n",
    "\n",
    "    \n",
    "def patient(soup):\n",
    "    # Extract relevant information (Patient Info)\n",
    "    patient_info = soup.find('div', class_='patient-card-wrap')\n",
    "    \n",
    "    # Check if the element is found\n",
    "    if patient_info:\n",
    "        patient_name = patient_info.find('p', class_='patient-card-info').text.strip()\n",
    "    \n",
    "        # Creating a DataFrame for Patient Info\n",
    "        df_patient = pd.DataFrame({'attribute': ['就诊患者'],\n",
    "                                   'value': [patient_name]})\n",
    "    \n",
    "    else:\n",
    "        print(\"Patient information not found.\")\n",
    "        df_patient = pd.DataFrame(columns=['attribute', 'value'])\n",
    "    df_patient = insert_cat(df_patient,'patient')\n",
    "    return df_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a231741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ...\n",
      "50 ...\n",
      "100 ...\n",
      "150 ...\n",
      "200 ...\n",
      "250 ...\n",
      "300 ...\n",
      "350 ...\n",
      "400 ...\n",
      "450 ...\n",
      "500 ...\n",
      "550 ...\n",
      "600 ...\n",
      "650 ...\n",
      "700 ...\n",
      "741 are saved,  9 failed.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(l_num)):\n",
    "    html_file_path = f'{outpath}/{l_num[i]}.html'\n",
    "    # Read the content of the HTML file\n",
    "    with open(html_file_path, 'r', encoding='utf-16') as file:\n",
    "               html_content = file.read()\n",
    "               \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    try:\n",
    "        df_meta = meta(soup)\n",
    "    \n",
    "        df_disease_info = disease_info(soup)\n",
    "    \n",
    "        df_suggestions = suggestions(soup)\n",
    "    \n",
    "        df_doctor_info =  doctor_info(soup)\n",
    "        \n",
    "        df_patient = patient(soup)\n",
    "    \n",
    "    \n",
    "        # Combine all DataFrames into one coherent DataFrame\n",
    "        df_combined = pd.concat([df_meta, df_disease_info, df_suggestions, df_doctor_info, df_patient],\n",
    "                           ignore_index=True)\n",
    "        # Print the resulting DataFrame\n",
    "        #print(df_combined)\n",
    "        save_dataframe(df_combined, l_num[i], outpath, \"info\")\n",
    "    except: \n",
    "        l_num_flaw.append(l_num[i])\n",
    "    if i%50==0:\n",
    "        print(i,'...')\n",
    "print(str(len(l_num)-len(np.unique(l_num_flaw))), 'are saved, ',len(np.unique(l_num_flaw)),\"failed.\")\n",
    "np.save(f'{outpath}/num_flaw.npy', np.unique(l_num_flaw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9c80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a5377df",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_num = [item for item in l_num if item not in l_num_flaw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c5eb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{outpath}/num.npy', l_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34049590-31f3-4402-b19a-4bd96021aa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ff1a4",
   "metadata": {},
   "source": [
    "### dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa9b77c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741\n"
     ]
    }
   ],
   "source": [
    "l_num = np.load(f'{outpath}/num.npy')\n",
    "print(len(l_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f56af261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue(soup):\n",
    "    messages = soup.find_all('div', class_='msg-item')\n",
    "    #print(len(messages))\n",
    "    \n",
    "    data = []\n",
    "    for message in messages:\n",
    "        time_element = message.find('div', class_='msg-time')\n",
    "        time = time_element.text.strip() if time_element else \"Unknown Time\"\n",
    "    \n",
    "        if 'item-left' in message['class'] or 'item-right' in message['class']:\n",
    "            sender_span = message.find('span', class_='content-name')\n",
    "            sender = sender_span.text.strip() if sender_span else \"System\"  # Default to \"System\" if sender is not found\n",
    "    \n",
    "            # Check for different message types\n",
    "            content_span = message.find('span', class_='content-text')\n",
    "            if content_span:\n",
    "                content = content_span.text.strip()\n",
    "            else:\n",
    "                # Handle other types of messages (e.g., life advice, diagnosis, system notices)\n",
    "                content_com_title = message.find('span', class_='content-com-title')\n",
    "                content_com_text = message.find('span', class_='content-com-text')\n",
    "                if content_com_title and content_com_text:\n",
    "                    content = f'{content_com_title.text.strip()}: {content_com_text.text.strip()}'\n",
    "                elif content_com_title:\n",
    "                    content = content_com_title.text.strip()\n",
    "                elif content_com_text:\n",
    "                    content = content_com_text.text.strip()\n",
    "                else:\n",
    "                    content = \"Unknown message type\"\n",
    "    \n",
    "            data.append({'Time': time, 'Sender': sender, 'Content': content})\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb182232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = [9,12,14,30,32,35,44,55,57,60,71,95,102,107,113,119,133,139,163,170,181,187,194,197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71852382",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_num_flaw = np.load(f'{outpath}/num_flaw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b71eb8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ...\n",
      "50 ...\n",
      "100 ...\n",
      "150 ...\n",
      "200 ...\n",
      "250 ...\n",
      "300 ...\n",
      "350 ...\n",
      "400 ...\n",
      "450 ...\n",
      "500 ...\n",
      "550 ...\n",
      "600 ...\n",
      "650 ...\n",
      "700 ...\n",
      "732 are saved,  9 failed.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(l_num)):\n",
    "    html_file_path = f'{outpath}/{l_num[i]}.html'\n",
    "    # Read the content of the HTML file\n",
    "    with open(html_file_path, 'r', encoding='utf-16') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    try:\n",
    "        df = dialogue(soup)\n",
    "    \n",
    "        save_dataframe(df, l_num[i], outpath,\"dialogue\")\n",
    "    except:\n",
    "        l_num_flaw.append(l_num[i])\n",
    "    if i%50==0:\n",
    "        print(i,'...')\n",
    "print(str(len(l_num)-len(np.unique(l_num_flaw))), 'are saved, ',len(np.unique(l_num_flaw)),\"failed.\")\n",
    "np.save(f'{outpath}/num_flaw.npy', np.unique(l_num_flaw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffa084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18324bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_num_flaw = np.load(f'{outpath}/num_flaw.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c38f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8897283364' '8897283508' '8897283756' '8897285638' '8897286429'\n",
      " '8897287010' '8897287389' '8897287926' '8897288332']\n"
     ]
    }
   ],
   "source": [
    "print(l_num_flaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33ac8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ec1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfe32a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b01ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8d998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfef747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4ac69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553b44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090d04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47672cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28bee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa54b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5188ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb9829e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2364a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28373bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b17b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136fb799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf06cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101eb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
